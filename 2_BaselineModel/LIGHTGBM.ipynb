{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Model Choice](#model-choice)\n",
    "2. [Feature Selection](#feature-selection)\n",
    "3. [Implementation](#implementation)\n",
    "4. [Evaluation](#evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "# Import your chosen baseline model\n",
    "# Example: from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load the training data\n",
    "df_raw_train = pd.read_csv('../data/train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Choice\n",
    "\n",
    "[Explain why you've chosen a particular model as the baseline. This could be a simple statistical model or a basic machine learning model. Justify your choice.]\n",
    "\n",
    "As a baseline model a very simple decison tree is implemented. The decision tree is well interpretable, making it possible to check for errors in the dataset, model and evaluationn pipeline. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "[Indicate which features from the dataset you will be using for the baseline model, and justify your selection.]\n",
    "\n",
    "Only Heart Rate (HR), Body Temperature (Temp), Respiratory Rate (Resp) and Systolic Blood Pressure (SBP) will be used for prediction as they are readily available and already routinely used in clinical practice (See qSOFA or SIRS [Please Note: WBC altough part of SIRS criteria is not taken into account as only available in <10% of data points]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17004, number of negative: 927794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6987\n",
      "[LightGBM] [Info] Number of data points in the train set: 944798, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017997 -> initscore=-3.999361\n",
      "[LightGBM] [Info] Start training from score -3.999361\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.794879\n",
      "[200]\tvalid_0's auc: 0.794406\n",
      "[300]\tvalid_0's auc: 0.793232\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's auc: 0.795586\n",
      "Fold 1: AUROC=0.7956  AUPRC=0.0852\n",
      "[LightGBM] [Info] Number of positive: 16985, number of negative: 927378\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7005\n",
      "[LightGBM] [Info] Number of data points in the train set: 944363, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017986 -> initscore=-4.000031\n",
      "[LightGBM] [Info] Start training from score -4.000031\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.800509\n",
      "[200]\tvalid_0's auc: 0.79854\n",
      "[300]\tvalid_0's auc: 0.796188\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's auc: 0.801015\n",
      "Fold 2: AUROC=0.8010  AUPRC=0.1014\n",
      "[LightGBM] [Info] Number of positive: 17000, number of negative: 926740\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6992\n",
      "[LightGBM] [Info] Number of data points in the train set: 943740, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018013 -> initscore=-3.998460\n",
      "[LightGBM] [Info] Start training from score -3.998460\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.809551\n",
      "[200]\tvalid_0's auc: 0.810948\n",
      "[300]\tvalid_0's auc: 0.809432\n",
      "[400]\tvalid_0's auc: 0.80643\n",
      "Early stopping, best iteration is:\n",
      "[231]\tvalid_0's auc: 0.811337\n",
      "Fold 3: AUROC=0.8113  AUPRC=0.1010\n",
      "[LightGBM] [Info] Number of positive: 16988, number of negative: 925607\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7003\n",
      "[LightGBM] [Info] Number of data points in the train set: 942595, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018023 -> initscore=-3.997943\n",
      "[LightGBM] [Info] Start training from score -3.997943\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.80111\n",
      "[200]\tvalid_0's auc: 0.80083\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's auc: 0.801476\n",
      "Fold 4: AUROC=0.8015  AUPRC=0.0951\n",
      "[LightGBM] [Info] Number of positive: 17011, number of negative: 928157\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6993\n",
      "[LightGBM] [Info] Number of data points in the train set: 945168, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017998 -> initscore=-3.999341\n",
      "[LightGBM] [Info] Start training from score -3.999341\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's auc: 0.800714\n",
      "[200]\tvalid_0's auc: 0.799401\n",
      "[300]\tvalid_0's auc: 0.794437\n",
      "Early stopping, best iteration is:\n",
      "[117]\tvalid_0's auc: 0.801495\n",
      "Fold 5: AUROC=0.8015  AUPRC=0.1011\n",
      "\n",
      "Mean AUROC=0.8022 ± 0.0051\n",
      "Mean AUPRC=0.0967 ± 0.0063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "DATA_DIR = \"/teamspace/studios/this_studio/detecting_Sepsis/data/!LIGHTGBM_DATA\"\n",
    "df = pd.read_csv(f\"{DATA_DIR}/train_fit.csv\")\n",
    "\n",
    "# cleanup\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "TARGET = \"SepsisLabel\"\n",
    "GROUP  = \"Patient_ID\"\n",
    "\n",
    "# optional stable ordering\n",
    "sort_cols = [c for c in [GROUP, \"ICULOS\", \"Hour\"] if c in df.columns]\n",
    "df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "# patient-level stratification label\n",
    "patient_y = df.groupby(GROUP)[TARGET].max().astype(int)\n",
    "patient_ids = patient_y.index.to_numpy()\n",
    "patient_labels = patient_y.values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(patient_ids, patient_labels), 1):\n",
    "    tr_pids = set(patient_ids[tr_idx])\n",
    "    va_pids = set(patient_ids[va_idx])\n",
    "\n",
    "    tr = df[df[GROUP].isin(tr_pids)]\n",
    "    va = df[df[GROUP].isin(va_pids)]\n",
    "\n",
    "    y_tr = tr[TARGET].astype(int).to_numpy()\n",
    "    y_va = va[TARGET].astype(int).to_numpy()\n",
    "\n",
    "    X_tr = tr.drop(columns=[TARGET, GROUP])\n",
    "    X_va = va.drop(columns=[TARGET, GROUP])\n",
    "\n",
    "    # imbalance weight (row-level)\n",
    "    pos = (y_tr == 1).sum()\n",
    "    neg = (y_tr == 0).sum()\n",
    "    spw = neg / max(pos, 1)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        metric =\"auc\",\n",
    "        n_estimators=4000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        min_child_samples=50,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=spw,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(100)]\n",
    "    )\n",
    "\n",
    "    p_va = model.predict_proba(X_va)[:, 1]\n",
    "    auroc = roc_auc_score(y_va, p_va)\n",
    "    auprc = average_precision_score(y_va, p_va)\n",
    "\n",
    "    fold_metrics.append((auroc, auprc))\n",
    "    print(f\"Fold {fold}: AUROC={auroc:.4f}  AUPRC={auprc:.4f}\")\n",
    "\n",
    "m = np.array(fold_metrics)\n",
    "print(f\"\\nMean AUROC={m[:,0].mean():.4f} ± {m[:,0].std():.4f}\")\n",
    "print(f\"Mean AUPRC={m[:,1].mean():.4f} ± {m[:,1].std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train_fit rows: 1180166 pos: 21247 neg: 1158919 scale_pos_weight: 54.54506518567327\n",
      "[LightGBM] [Info] Number of positive: 21247, number of negative: 1158919\n",
      "[LightGBM] [Info] Total Bins 7021\n",
      "[LightGBM] [Info] Number of data points in the train set: 1180166, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018003 -> initscore=-3.999027\n",
      "[LightGBM] [Info] Start training from score -3.999027\n",
      "Saved final model to: /teamspace/studios/this_studio/detecting_Sepsis/data/!LIGHTGBM_DATA/lgbm_final_122trees.txt\n",
      "Saved feature names to: /teamspace/studios/this_studio/detecting_Sepsis/data/!LIGHTGBM_DATA/lgbm_feature_names.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "DATA_DIR = \"/teamspace/studios/this_studio/detecting_Sepsis/data/!LIGHTGBM_DATA\"\n",
    "TRAIN_PATH = f\"{DATA_DIR}/train_fit.csv\"\n",
    "\n",
    "TARGET = \"SepsisLabel\"\n",
    "GROUP  = \"Patient_ID\"\n",
    "BEST_N = 122  # median best_iteration from your 5-fold CV\n",
    "\n",
    "# -------------------\n",
    "# Load + cleanup\n",
    "# -------------------\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# stable order (optional)\n",
    "sort_cols = [c for c in [GROUP, \"ICULOS\", \"Hour\"] if c in df.columns]\n",
    "df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "# -------------------\n",
    "# Features / target\n",
    "# -------------------\n",
    "y_full = df[TARGET].astype(int).to_numpy()\n",
    "X_full = df.drop(columns=[TARGET, GROUP])\n",
    "\n",
    "# class imbalance weight on FULL train_fit\n",
    "pos = int((y_full == 1).sum())\n",
    "neg = int((y_full == 0).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)\n",
    "print(\"Full train_fit rows:\", len(df), \"pos:\", pos, \"neg:\", neg, \"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# -------------------\n",
    "# Final model (no early stopping)\n",
    "# -------------------\n",
    "final_model = lgb.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    metric=\"auc\",\n",
    "    n_estimators=BEST_N,        # <-- fixed based on CV\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    min_child_samples=50,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    force_row_wise=True,       # avoid the overhead message\n",
    ")\n",
    "\n",
    "final_model.fit(X_full, y_full)\n",
    "\n",
    "# -------------------\n",
    "# Save the model\n",
    "# -------------------\n",
    "MODEL_PATH = os.path.join(DATA_DIR, f\"lgbm_final_{BEST_N}trees.txt\")\n",
    "final_model.booster_.save_model(MODEL_PATH)\n",
    "print(\"Saved final model to:\", MODEL_PATH)\n",
    "\n",
    "# Optional: save feature names (useful later)\n",
    "FEAT_PATH = os.path.join(DATA_DIR, \"lgbm_feature_names.txt\")\n",
    "with open(FEAT_PATH, \"w\") as f:\n",
    "    for c in X_full.columns:\n",
    "        f.write(c + \"\\n\")\n",
    "print(\"Saved feature names to:\", FEAT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LightGBM -> Official evaluator threshold sweep (EXACT)\n",
    "# Fixes PSV/train_fit feature mismatch (Hour vs ICULOS, etc.)\n",
    "# Writes predictions to your folder and calls evaluate_sepsis_score.py\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (edit if needed)\n",
    "# -----------------------------\n",
    "EVAL_SCRIPT = \"/teamspace/studios/this_studio/detecting_Sepsis/4_Evaluation/evaluate_sepsis_score.py\"\n",
    "\n",
    "DATA_DIR = \"/teamspace/studios/this_studio/detecting_Sepsis/data/!LIGHTGBM_DATA\"\n",
    "LABEL_DIR_THRESH = f\"{DATA_DIR}/PSV_Patients_THRESH\"\n",
    "LABEL_DIR_TEST   = f\"{DATA_DIR}/PSV_Patients_TEST\"\n",
    "\n",
    "PRED_ROOT = \"/teamspace/studios/this_studio/detecting_Sepsis/4_Evaluation/Predictions/LIGHTGBM\"\n",
    "PRED_DIR_THRESH_WORK = f\"{PRED_ROOT}/THRESH_WORK\"   # overwritten each threshold\n",
    "PRED_DIR_TEST_FINAL  = f\"{PRED_ROOT}/TEST_FINAL\"\n",
    "\n",
    "MODEL_PATH = f\"{DATA_DIR}/lgbm_final_122trees.txt\"\n",
    "FEATURES_PATH = f\"{DATA_DIR}/lgbm_feature_names.txt\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load official evaluator (exact)\n",
    "# -----------------------------\n",
    "def load_official_evaluator(eval_script_path: str):\n",
    "    spec = importlib.util.spec_from_file_location(\"eval_sepsis\", eval_script_path)\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model + features\n",
    "# -----------------------------\n",
    "def load_booster(model_path: str) -> lgb.Booster:\n",
    "    return lgb.Booster(model_file=model_path)\n",
    "\n",
    "def load_feature_list(path: str) -> list[str]:\n",
    "    with open(path, \"r\") as f:\n",
    "        feats = [line.strip() for line in f if line.strip()]\n",
    "    return feats\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# IO helpers\n",
    "# -----------------------------\n",
    "def list_patient_files(psv_dir: str) -> list[Path]:\n",
    "    return sorted(Path(psv_dir).glob(\"*.psv\"))\n",
    "\n",
    "def ensure_empty_dir(dir_path: str):\n",
    "    d = Path(dir_path)\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    for f in d.glob(\"*.psv\"):\n",
    "        f.unlink()\n",
    "\n",
    "def write_prediction_file(out_path: Path, prob: np.ndarray, threshold: float):\n",
    "    pred = (prob >= threshold).astype(int)\n",
    "    out_df = pd.DataFrame({\n",
    "        \"PredictedProbability\": prob,\n",
    "        \"PredictedLabel\": pred\n",
    "    })\n",
    "    out_df.to_csv(out_path, sep=\"|\", index=False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Feature alignment: PSV -> model features\n",
    "# -----------------------------\n",
    "def prepare_features_from_psv(df: pd.DataFrame, feature_cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make a PSV dataframe compatible with the model's expected features:\n",
    "      - Drop SepsisLabel if present (never use as feature)\n",
    "      - If model expects Hour but PSV doesn't have it, derive Hour = ICULOS - 1\n",
    "      - If model expects ICULOS but PSV doesn't have it, derive ICULOS = Hour + 1\n",
    "      - Add any other missing expected columns as NaN\n",
    "      - Return columns in EXACT training order\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=[\"SepsisLabel\"], errors=\"ignore\")\n",
    "\n",
    "    # Derive Hour / ICULOS if needed\n",
    "    if \"Hour\" in feature_cols and \"Hour\" not in df.columns:\n",
    "        if \"ICULOS\" in df.columns:\n",
    "            df[\"Hour\"] = df[\"ICULOS\"] - 1\n",
    "        else:\n",
    "            df[\"Hour\"] = np.nan\n",
    "\n",
    "    if \"ICULOS\" in feature_cols and \"ICULOS\" not in df.columns:\n",
    "        if \"Hour\" in df.columns:\n",
    "            df[\"ICULOS\"] = df[\"Hour\"] + 1\n",
    "        else:\n",
    "            df[\"ICULOS\"] = np.nan\n",
    "\n",
    "    # Add missing model features as NaN\n",
    "    for c in feature_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    return df[feature_cols]\n",
    "\n",
    "def read_features_from_patient_file(psv_path: Path, feature_cols: list[str]) -> pd.DataFrame:\n",
    "    df = pd.read_csv(psv_path, sep=\"|\")\n",
    "    return prepare_features_from_psv(df, feature_cols)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Core: cache probabilities once (per split)\n",
    "# -----------------------------\n",
    "def cache_probabilities(label_dir: str, booster: lgb.Booster, feature_cols: list[str]) -> dict[str, np.ndarray]:\n",
    "    probs = {}\n",
    "    files = list_patient_files(label_dir)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .psv files found in: {label_dir}\")\n",
    "\n",
    "    for fp in files:\n",
    "        X = read_features_from_patient_file(fp, feature_cols)\n",
    "        probs[fp.name] = booster.predict(X)  # prob per row\n",
    "    return probs\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Core: materialize predictions for a threshold\n",
    "# -----------------------------\n",
    "def materialize_predictions(probs: dict[str, np.ndarray], out_dir: str, threshold: float):\n",
    "    ensure_empty_dir(out_dir)\n",
    "    out = Path(out_dir)\n",
    "    for fname, p in probs.items():\n",
    "        write_prediction_file(out / fname, p, threshold)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate (official)\n",
    "# -----------------------------\n",
    "def official_eval(evaluator_module, label_dir: str, pred_dir: str):\n",
    "    # returns: auroc, auprc, accuracy, f_measure, normalized_observed_utility\n",
    "    return evaluator_module.evaluate_sepsis_score(label_dir, pred_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST t=0.010 | Utility=-0.60402 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.020 | Utility=-0.60313 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.030 | Utility=-0.59913 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.040 | Utility=-0.58524 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.050 | Utility=-0.56408 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.060 | Utility=-0.53880 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.070 | Utility=-0.50597 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.080 | Utility=-0.47070 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.090 | Utility=-0.43509 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.100 | Utility=-0.40085 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.110 | Utility=-0.36357 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.120 | Utility=-0.32933 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.130 | Utility=-0.29589 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.140 | Utility=-0.26317 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.150 | Utility=-0.22864 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.160 | Utility=-0.19593 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.170 | Utility=-0.16428 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.180 | Utility=-0.13085 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.190 | Utility=-0.10126 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.200 | Utility=-0.07578 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.210 | Utility=-0.05407 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.220 | Utility=-0.02725 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.230 | Utility=-0.00796 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.240 | Utility=0.00869 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.250 | Utility=0.02808 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.260 | Utility=0.04724 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.270 | Utility=0.05461 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.280 | Utility=0.07614 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.290 | Utility=0.09315 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.300 | Utility=0.11156 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.310 | Utility=0.13261 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.320 | Utility=0.14633 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.330 | Utility=0.15647 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.340 | Utility=0.16888 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.350 | Utility=0.18391 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.360 | Utility=0.19573 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.370 | Utility=0.20560 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.380 | Utility=0.22066 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.390 | Utility=0.23260 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.400 | Utility=0.23819 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.410 | Utility=0.24861 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.420 | Utility=0.26392 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.430 | Utility=0.26749 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.440 | Utility=0.28013 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.450 | Utility=0.28564 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.460 | Utility=0.28838 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.470 | Utility=0.29206 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.480 | Utility=0.30036 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.520 | Utility=0.30212 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.530 | Utility=0.30513 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.540 | Utility=0.30931 | AUROC=0.7806 AUPRC=0.0885\n",
      "NEW BEST t=0.550 | Utility=0.31446 | AUROC=0.7806 AUPRC=0.0885\n",
      "\n",
      "Best threshold found:\n",
      "(0.55, 0.3144573980279918, 0.7805540385569937, 0.08851108781385576, 0.8577715968586388, 0.11647525154995426)\n",
      "Saved sweep results to: /teamspace/studios/this_studio/detecting_Sepsis/4_Evaluation/Predictions/LIGHTGBM/thresh_sweep_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Run threshold sweep on THRESH (exact official scoring)\n",
    "# ============================================================\n",
    "Path(PRED_ROOT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "evaluator = load_official_evaluator(EVAL_SCRIPT)\n",
    "booster = load_booster(MODEL_PATH)\n",
    "feature_cols = load_feature_list(FEATURES_PATH)\n",
    "\n",
    "# Cache probabilities ONCE for THRESH (fast; no re-predicting for every threshold)\n",
    "probs_thresh = cache_probabilities(LABEL_DIR_THRESH, booster, feature_cols)\n",
    "\n",
    "thresholds = np.round(np.arange(0.01, 0.91, 0.01), 2) \n",
    "rows = []\n",
    "best = None\n",
    "\n",
    "for t in thresholds:\n",
    "    t = float(t)\n",
    "    materialize_predictions(probs_thresh, PRED_DIR_THRESH_WORK, t)\n",
    "    auroc, auprc, acc, f1, util = official_eval(evaluator, LABEL_DIR_THRESH, PRED_DIR_THRESH_WORK)\n",
    "\n",
    "    rows.append((t, util, auroc, auprc, acc, f1))\n",
    "\n",
    "    if best is None or util > best[1]:\n",
    "        best = (t, util, auroc, auprc, acc, f1)\n",
    "        print(f\"NEW BEST t={t:.3f} | Utility={util:.5f} | AUROC={auroc:.4f} AUPRC={auprc:.4f}\")\n",
    "\n",
    "results = pd.DataFrame(rows, columns=[\"threshold\",\"utility\",\"auroc\",\"auprc\",\"accuracy\",\"f1\"])\n",
    "out_csv = f\"{PRED_ROOT}/thresh_sweep_results.csv\"\n",
    "results.to_csv(out_csv, index=False)\n",
    "\n",
    "print(\"\\nBest threshold found:\")\n",
    "print(best)\n",
    "print(\"Saved sweep results to:\", out_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote TEST predictions to: /teamspace/studios/this_studio/detecting_Sepsis/4_Evaluation/Predictions/LIGHTGBM/TEST_FINAL\n",
      "Best threshold used: 0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# (Optional) Write FINAL TEST predictions using best threshold\n",
    "# ============================================================\n",
    "best_t = float(best[0])\n",
    "\n",
    "probs_test = cache_probabilities(LABEL_DIR_TEST, booster, feature_cols)\n",
    "materialize_predictions(probs_test, PRED_DIR_TEST_FINAL, best_t)\n",
    "\n",
    "print(\"\\nWrote TEST predictions to:\", PRED_DIR_TEST_FINAL)\n",
    "print(\"Best threshold used:\", best_t)\n",
    "\n",
    "# If your TEST folder has labels and you want to score locally (optional):\n",
    "# auroc, auprc, acc, f1, util = official_eval(evaluator, LABEL_DIR_TEST, PRED_DIR_TEST_FINAL)\n",
    "# print(\"TEST (official):\", {\"auroc\": auroc, \"auprc\": auprc, \"acc\": acc, \"f1\": f1, \"utility\": util})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST (official):\n",
      "AUROC    = 0.817455\n",
      "AUPRC    = 0.110526\n",
      "Accuracy = 0.848159\n",
      "F1       = 0.125706\n",
      "Utility  = 0.383143\n"
     ]
    }
   ],
   "source": [
    "auroc, auprc, acc, f1, util = official_eval(evaluator, LABEL_DIR_TEST, PRED_DIR_TEST_FINAL)\n",
    "\n",
    "print(\"TEST (official):\")\n",
    "print(f\"AUROC    = {auroc:.6f}\")\n",
    "print(f\"AUPRC    = {auprc:.6f}\")\n",
    "print(f\"Accuracy = {acc:.6f}\")\n",
    "print(f\"F1       = {f1:.6f}\")\n",
    "print(f\"Utility  = {util:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement your baseline model here.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the baseline model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "[Clearly state what metrics you will use to evaluate the model's performance. These metrics will serve as a starting point for evaluating more complex models later on.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model\n",
    "# Example for a classification problem\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# For a regression problem, you might use:\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Your evaluation code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
